# -*- coding: utf-8 -*-
"""suc_Qwen3-VL-30B_OCR_TRANSLATOR_llamacpp_CPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GFyjA3ru7zmtqECjdiPLMm5uFMv0s-e
"""



"""### Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù†Ù‡ ÙƒØ§Ù†Ù‡ ocr and translator"""



"""https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF

https://unsloth.ai/docs/models/qwen3-vl-how-to-run-and-fine-tune#running-qwen3-vl
"""



!wget https://github.com/ggml-org/llama.cpp/releases/download/b7613/llama-b7613-bin-ubuntu-x64.tar.gz

!tar -xzf llama-b7613-bin-ubuntu-x64.tar.gz

!wget https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg -O picture.png

!/content/llama-b7613/llama-mtmd-cli \
    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \
    --n-gpu-layers 99 \
    --jinja \
    --top-p 0.8 \
    --top-k 20 \
    --temp 0.7 \
    --min-p 0.0 \
    --flash-attn on \
    --presence-penalty 1.5 \
    --ctx-size 8192

"""

load_backend: loaded RPC backend from /content/llama-b7613/libggml-rpc.so
load_backend: loaded CPU backend from /content/llama-b7613/libggml-cpu-haswell.so
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf to /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf.downloadInProgress (server_etag:"6779d1c71c89dca5f9995ad2b050ee372b10218a7975ee1871f8527e92397c06", server_last_modified:)...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1351  100  1351    0     0   6962      0 --:--:-- --:--:-- --:--:--  6962
100 4910M  100 4910M    0     0   169M      0  0:00:29  0:00:29 --:--:--  210M
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_mmproj-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj-F16.gguf to /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_mmproj-F16.gguf.downloadInProgress (server_etag:"bfd8f6b41ee1cffff4d018a40a00c8b9586207830586db0996402c1a37d55956", server_last_modified:)...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1319  100  1319    0     0   5565      0 --:--:-- --:--:-- --:--:--  5565
100 1105M  100 1105M    0     0   164M      0  0:00:06  0:00:06 --:--:--  139M
build: 7613 (f38de1634) with GNU 11.4.0 for Linux x86_64
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
llama_params_fit_impl: no devices with dedicated memory found
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 0.54 seconds
llama_model_loader: loaded meta data with 42 key-value pairs and 399 tensors from /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3vl
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-Vl-8B-Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen3-Vl-8B-Instruct
llama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   6:                         general.size_label str              = 8B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   9:                   general.base_model.count u32              = 1
llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen3 VL 8B Instruct
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-VL-...
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["unsloth", "image-text-to-text"]
llama_model_loader: - kv  14:                        qwen3vl.block_count u32              = 36
llama_model_loader: - kv  15:                     qwen3vl.context_length u32              = 262144
llama_model_loader: - kv  16:                   qwen3vl.embedding_length u32              = 4096
llama_model_loader: - kv  17:                qwen3vl.feed_forward_length u32              = 12288
llama_model_loader: - kv  18:               qwen3vl.attention.head_count u32              = 32
llama_model_loader: - kv  19:            qwen3vl.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                     qwen3vl.rope.freq_base f32              = 5000000.000000
llama_model_loader: - kv  21:   qwen3vl.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:               qwen3vl.attention.key_length u32              = 128
llama_model_loader: - kv  23:             qwen3vl.attention.value_length u32              = 128
llama_model_loader: - kv  24:            qwen3vl.rope.dimension_sections arr[i32,4]       = [24, 20, 20, 0]
llama_model_loader: - kv  25:                 qwen3vl.n_deepstack_layers u32              = 3
llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama_model_loader: - kv  37:                          general.file_type u32              = 15
llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-VL-8B-Instruct-GGUF/imatrix_uns...
llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-VL-8B-Instr...
llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 694
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type q4_K:  153 tensors
llama_model_loader: - type q5_K:   25 tensors
llama_model_loader: - type q6_K:   56 tensors
llama_model_loader: - type iq4_xs:   20 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.79 GiB (5.02 BPW)
load: 0 unused tokens
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3vl
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 262144
print_info: n_embd           = 4096
print_info: n_embd_inp       = 16384
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 40
print_info: rope scaling     = linear
print_info: freq_base_train  = 5000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 262144
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: mrope sections   = [24, 20, 20, 0]
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3-Vl-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'ÄŠ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 35 repeating layers to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:   CPU_Mapped model buffer size =  4904.50 MiB
load_tensors:   CPU_REPACK model buffer size =  2344.50 MiB
......................................................................................
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|im_end|> logit bias = -inf
common_init_result: added <|fim_pad|> logit bias = -inf
common_init_result: added <|repo_name|> logit bias = -inf
common_init_result: added <|file_sep|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 5000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (8192) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.58 MiB
llama_kv_cache:        CPU KV buffer size =  1152.00 MiB
llama_kv_cache: size = 1152.00 MiB (  8192 cells,  36 layers,  1/1 seqs), K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_context:        CPU compute buffer size =   312.75 MiB
llama_context: graph nodes  = 1267
llama_context: graph splits = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
mtmd_cli_context: chat template example:
<|im_start|>system
You are a helpful assistant<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
Hi there<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant

clip_model_loader: model name:   Qwen3-Vl-8B-Instruct
clip_model_loader: description:  
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    352
clip_model_loader: n_kv:         31

clip_model_loader: has vision encoder
clip_ctx: CLIP using CPU backend
load_hparams: Qwen-VL models require at minimum 1024 image tokens to function correctly on grounding tasks
load_hparams: if you encounter problems with accuracy, try adding --image-min-tokens 1024
load_hparams: more info: https://github.com/ggml-org/llama.cpp/issues/16842

load_hparams: projector:          qwen3vl_merger
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            27
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     4096

--- vision hparams ---
load_hparams: image_size:         768
load_hparams: patch_size:         16
load_hparams: has_llava_proj:     0
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            2
load_hparams: n_wa_pattern: 0
load_hparams: image_min_pixels:   8192
load_hparams: image_max_pixels:   4194304

load_hparams: model size:         1105.32 MiB
load_hparams: metadata size:      0.12 MiB
warmup: warmup with image size = 1472 x 1472
alloc_compute_meta:        CPU compute buffer size =   372.08 MiB
alloc_compute_meta: graph splits = 1, nodes = 853
warmup: flash attention is enabled
main: loading model: /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf
WARN: This is an experimental CLI for testing multimodal capability.
      For normal use cases, please use the standard llama-cli

 Running in chat mode, available commands:
   /image <path>    load an image
   /clear           clear the chat history
   /quit or /exit   exit the program

> hi

Hello! ğŸ˜Š  
How can I assist you today? Whether you have a question, need help with something, or just want to chatâ€”I'm here for you.

> /image /content/picture.png
/content/picture.png image loaded

> What is this image?
encoding image slice...
image slice encoded in 393800 ms
decoding image batch 1/1, n_tokens_batch = 589
image decoded (batch 1/1) in 266328 ms

This image shows a **three-toed sloth** (*Bradypus variegatus*), one of the two species of sloths native to Central and South America.

Here are some key features visible in the photo:

- **Appearance**: It has long, shaggy fur that is brownish-gray, which helps camouflage it in the trees. The fur also hosts symbiotic algae, giving it a greenish tint in some areas.
- **Face**: Its face is characterized by dark eyes and a small, rounded nose. You can see a faint white or light-colored patch around its muzzle.
- **Posture**: The sloth is hanging from

"""



from IPython.display import Image
Image('/content/picture.png')

"""https://unsloth.ai/docs/models/qwen3-vl-how-to-run-and-fine-tune#running-qwen3-vl"""

!/content/llama-b7613/llama-mtmd-cli -h

"""### Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù†Ù‡ ÙƒØ§Ù†Ù‡ ocr and translator"""

!/content/llama-b7613/llama-mtmd-cli \
    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \
    --n-gpu-layers 99 \
    --jinja \
    --top-p 0.8 \
    --top-k 20 \
    --temp 0.7 \
    --min-p 0.0 \
    --flash-attn on \
    --presence-penalty 1.5 \
    --ctx-size 8192



"""
Ø£ÙˆÙ‚ÙÙ†Ø§ ÙÙŠ ÙˆÙ‚Øª Ø¬ÙŠØ¯ Ø¬Ø¯Ù‹Ø§ØŒ ÙˆÙˆØµÙ„Ù†Ø§ Ø¨Ø¹Ø¯ ØºØ±ÙˆØ¨ Ø§Ù„Ø´Ù…Ø³ Ø¥Ù„Ù‰ ÙƒÙ„Ø§ÙˆØ³ÙŠÙ†Ø¨ÙˆØ±Øº. Ù„Ù‚Ø¯ ØªÙˆÙ‚ÙØª Ù„Ù„ÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ ÙÙ†Ø¯Ù‚ Ø±ÙˆÙŠØ§Ù„. ÙƒØ§Ù† Ø§Ù„Ø¹Ø´Ø§Ø¡ØŒ Ø£Ùˆ Ø§Ù„ØºØ¯Ø§Ø¡ØŒ Ø¯Ø¬Ø§Ø¬Ù‹Ø§ Ù…Ø·Ù‡ÙˆÙ‹Ø§ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø§ Ù…Ø¹ Ø§Ù„ÙÙ„ÙÙ„ Ø§Ù„Ø£Ø­Ù…Ø±ØŒ ÙˆÙ‡Ùˆ Ø·Ø¹Ø§Ù… Ø±Ø§Ø¦Ø¹ Ø¬Ø¯Ù‹Ø§. (Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØµÙØ© Ù…Ù† Ù…ÙŠÙ†Ø§.) Ø³Ø£Ù„Øª Ø§Ù„Ø®Ø§Ø¯Ù…ØŒ ÙˆÙ‚Ø§Ù„ Ù„ÙŠ Ø¥Ù†Ù‡ ÙŠØ³Ù…Ù‰ "Ø¨Ø§Ø¨Ø±ÙŠÙƒØ§ Ù‡Ù†Ø¯Ù„"ØŒ ÙˆØ£Ù†Ù‡ Ø·Ø¨Ù‚ ÙˆØ·Ù†ÙŠØŒ Ø¥Ø°Ù‹Ø§ ÙŠØ¬Ø¨ Ø£Ù† Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù† Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„"""

!/content/llama-b7613/llama-mtmd-cli \
    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \
    --n-gpu-layers 99 \
    --jinja \
    --top-p 0.8 \
    --top-k 20 \
    --temp 0.7 \
    --min-p 0.0 \
    --flash-attn on \
    --presence-penalty 1.5 \
    --ctx-size 8192

import glob
import os

# Get all PNG files in the /content/ directory
image_files = sorted(glob.glob('/content/*.png'))

# Format them as a comma-separated string
image_paths_str = ",".join(image_files)

# Construct the full command (remove '!' prefix)
command = f"""/content/llama-b7613/llama-mtmd-cli \
    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \
    --n-gpu-layers 99 \
    --jinja \
    --top-p 0.8 \
    --top-k 20 \
    --temp 0.7 \
    --min-p 0.0 \
    --flash-attn on \
    --presence-penalty 1.5 \
    --ctx-size 8192 \
    --image \"{image_paths_str}\" \
    --image-min-tokens 1024 \
    -p \"What is in these images?\""""

# Execute the command
print(f"Executing command:\n{command}")
get_ipython().system(command)

"""### ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ØµÙˆØ±ØªØ§Ù†"""

!/content/llama-b7613/llama-mtmd-cli \
    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \
    --n-gpu-layers 99 \
    --jinja \
    --top-p 0.8 \
    --top-k 20 \
    --temp 0.7 \
    --min-p 0.0 \
    --flash-attn on \
    --presence-penalty 1.5 \
    --ctx-size 8192 \
    --image "/content/1.png,/content/11.png" \
    -p "What is in these images?"

"""

ip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    352
clip_model_loader: n_kv:         31

clip_model_loader: has vision encoder
clip_ctx: CLIP using CPU backend
load_hparams: Qwen-VL models require at minimum 1024 image tokens to function correctly on grounding tasks
load_hparams: if you encounter problems with accuracy, try adding --image-min-tokens 1024
load_hparams: more info: https://github.com/ggml-org/llama.cpp/issues/16842

load_hparams: projector:          qwen3vl_merger
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            27
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     4096

--- vision hparams ---
load_hparams: image_size:         768
load_hparams: patch_size:         16
load_hparams: has_llava_proj:     0
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            2
load_hparams: n_wa_pattern: 0
load_hparams: image_min_pixels:   8192
load_hparams: image_max_pixels:   4194304

load_hparams: model size:         1105.32 MiB
load_hparams: metadata size:      0.12 MiB
warmup: warmup with image size = 1472 x 1472
alloc_compute_meta:        CPU compute buffer size =   372.08 MiB
alloc_compute_meta: graph splits = 1, nodes = 853
warmup: flash attention is enabled
main: loading model: /root/.cache/llama.cpp/unsloth_Qwen3-VL-8B-Instruct-GGUF_Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf
WARN: This is an experimental CLI for testing multimodal capability.
      For normal use cases, please use the standard llama-cli
encoding image slice...
image slice encoded in 16818 ms
decoding image batch 1/1, n_tokens_batch = 80
image decoded (batch 1/1) in 33315 ms
encoding image slice...
image slice encoded in 291499 ms
decoding image batch 1/1, n_tokens_batch = 500
image decoded (batch 1/1) in 231194 ms

The two images contain excerpts from different texts.

**Image 1:**
This is a passage from a travelogue or memoir. It describes the author's experience in Klausenburg (now Cluj-Napoca, Romania). Key points include:
*   The author arrived after nightfall and stayed overnight at the Hotel Royale.
*   For dinner, they had "a chicken done up some way with red pepper," which they found very good but made them thirsty.
*   They asked the waiter about the dish, who identified it as "paprika hendl" and noted that it is a national dish of the Carpathian region.

**Image 2:**
This is an excerpt from Chief Seattle's famous speech, delivered in 1854. It is a powerful statement on the spiritual connection of Native Americans to the land and their opposition to selling it.
*   It begins by questioning how one can buy or sell the earth, air, water, or other natural elements, which are sacred.
*   It describes the land and its features as belonging to the people's memory and experience, including "every shining pine needle," "every sandy shore," "every mist," "every clearing," and "every humming insect."
*   The speech emphasizes that the land is sacred and that selling it requires teaching future generations to respect this sacredness.
*   It concludes with a statement of cultural difference: "Our ways are different from you ways."

In summary, Image 1 is about a traveler's culinary experience in Romania, while Image 2 is a profound indigenous perspective on land ownership and environmental stewardship.


llama_perf_context_print:        load time =    8677.42 ms
llama_perf_context_print: prompt eval time =  586370.08 ms /   598 tokens (  980.55 ms per token,     1.02 tokens per second)
llama_perf_context_print:        eval time =  315122.46 ms /   326 runs   (  966.63 ms per token,     1.03 tokens per second)
llama_perf_context_print:       total time =  905675.28 ms /   924 tokens
llama_perf_context_print:    graphs reused =          0"""

/content/1.png,/content/11.png

from IPython.display import Image
Image('/content/*.png')